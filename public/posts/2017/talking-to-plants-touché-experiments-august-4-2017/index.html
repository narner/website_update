<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Nick Arner  | Talking to Plants: Touché Experiments</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.68.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.1cb140d8ba31d5b2f1114537dd04802a.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Talking to Plants: Touché Experiments" />
<meta property="og:description" content="As I mentioned in a previous post, I was really pleased to see that the ESP-Sensors project had included code for working with a circuit based on Touché.
I had earlier come across other implementations of Touché for the Arduino, but unlike the ESP project, none of them utilized machine learning for classifying gesture types.
Touché is a project developed at Disney Research that uses swept-frequency capacitive sensing to &ldquo;&hellip;not only detect a touch event, but simultaneously recognize complex configurations of the human hands and body during touch interaction." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/2017/talking-to-plants-touche%CC%81-experiments-august-4-2017/" />
<meta property="article:published_time" content="2017-08-04T00:00:00+00:00" />
<meta property="article:modified_time" content="2017-08-04T00:00:00+00:00" />
<meta itemprop="name" content="Talking to Plants: Touché Experiments">
<meta itemprop="description" content="As I mentioned in a previous post, I was really pleased to see that the ESP-Sensors project had included code for working with a circuit based on Touché.
I had earlier come across other implementations of Touché for the Arduino, but unlike the ESP project, none of them utilized machine learning for classifying gesture types.
Touché is a project developed at Disney Research that uses swept-frequency capacitive sensing to &ldquo;&hellip;not only detect a touch event, but simultaneously recognize complex configurations of the human hands and body during touch interaction.">
<meta itemprop="datePublished" content="2017-08-04T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2017-08-04T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1220">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Talking to Plants: Touché Experiments"/>
<meta name="twitter:description" content="As I mentioned in a previous post, I was really pleased to see that the ESP-Sensors project had included code for working with a circuit based on Touché.
I had earlier come across other implementations of Touché for the Arduino, but unlike the ESP project, none of them utilized machine learning for classifying gesture types.
Touché is a project developed at Disney Research that uses swept-frequency capacitive sensing to &ldquo;&hellip;not only detect a touch event, but simultaneously recognize complex configurations of the human hands and body during touch interaction."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="http://example.org/" class="f3 fw2 hover-white no-underline white-90 dib">
      Nick Arner
    </a>
    <div class="flex-l items-center">
      

      
      














    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=http://example.org/posts/2017/talking-to-plants-touche%CC%81-experiments-august-4-2017/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=http://example.org/posts/2017/talking-to-plants-touche%CC%81-experiments-august-4-2017/&amp;text=Talking%20to%20Plants:%20Touch%c3%a9%20Experiments" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://example.org/posts/2017/talking-to-plants-touche%CC%81-experiments-august-4-2017/&amp;title=Talking%20to%20Plants:%20Touch%c3%a9%20Experiments" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>

      <h1 class="f1 athelas mt3 mb1">Talking to Plants: Touché Experiments</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2017-08-04T00:00:00Z">August 4, 2017</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>As I mentioned in <a href="https://www.nickarner.com/blog/tools-for-machine-learning-and-sensor-inputs-for-gesture-recognition">a previous post</a>, I was really pleased to see that the <a href="https://github.com/damellis/ESP">ESP-Sensors project</a> had included code for <a href="https://github.com/damellis/ESP/wiki/%5BExample%5D-Touch%C3%A9-swept-frequency-capacitive-sensing">working with a circuit based on Touché</a>.</p>
<p>I had earlier come across other implementations of Touché for the Arduino, but unlike the ESP project, none of them utilized machine learning for classifying gesture types.</p>
<p>Touché is a <a href="https://www.disneyresearch.com/project/touche-touch-and-gesture-sensing-for-the-real-world/">project developed at Disney Research</a> that uses swept-frequency capacitive sensing to &ldquo;&hellip;not only detect a touch event, but simultaneously recognize complex configurations of the human hands and body during touch interaction.&rdquo;</p>
<p>In other words, it&rsquo;s able to not just tell <em>if</em> a touch event occurred, but what <em>type</em> of touch event occurred. This differs from most capacitive sensors, which are only able to detect whether a touch event occurred, and possibly how far away from a sensor the user&rsquo;s hand is.</p>
<p>Traditional capacitive sensing works by generating an electrical signal at a single frequency. This signal is applied to a conductive surface, such as a metal plate. When a hand is either close to or touching the surface, the value of capacitance changes - signifying that a touch event has occurred, or that a hand is close to the surface of the sensor. The <a href="http://playground.arduino.cc/Main/CapacitiveSensor?from=Main.CapSense">CapSense library</a> allows for traditional capacitive sensing to be implemented on an Arduino.</p>
<p>Swept-frequency capacitive sensing makes use of <em>multiple</em> frequencies. In their <a href="https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20140805145650/touchechi20121.pdf">C</a><a href="https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20140805145650/touchechi20121.pdf">HI 2012 paper</a>, the Touché developers state the reason for using multiple frequencies is &ldquo;Objects excited by an electrical signal respond differently at different frequencies, therefore, the changes in the return signal will also be frequency dependent.&rdquo; Rather than using a single data point generated by an electrical signal at a single frequency, as in traditional capacitive sensing, Touché utilizes multiple data points from multiple generated frequencies.</p>
<p>This capacitive profile is used to train a machine learning pipeline to differentiate between various touch interactions. This machine learning pipeline is based around a Support Vector Machine. Specifically, it uses the <a href="http://www.nickgillian.com/wiki/pmwiki.php?n=GRT.SVM">SVM module from the Gesture Recognition Toolkit</a>.</p>
<p>Check out the video that accompanied the Touché CHI 2012 paper:</p>
<p><a href="http://www.youtube.com/watch?v=E4tYpXVTjxA" title=""><img src="http://img.youtube.com/vi/E4tYpXVTjxA/0.jpg" alt=""></a></p>
<p>There have been a few other open-source implementation of touch-sensing based off of Touché that I&rsquo;ve come across before, but the one provided in the ESP project seemed to be the easiest to set-up, and the most usable to work with.</p>
<p>The authors continued their work by showing how Touché could be used to interact with plants in the <a href="https://www.disneyresearch.com/project/botanicus-interacticus-interactive-plant-technology/">Botanicus Interacticus project</a>, which was displayed in an exhibition at SIGGRAPH 2012:</p>
<p><a href="http://www.youtube.com/watch?v=EcRSKEIucjk" title=""><img src="http://img.youtube.com/vi/EcRSKEIucjk/0.jpg" alt=""></a></p>
<p>I have two plants on my desk: a fern plant, and an air plant. I really enjoy the way they add some color to my work area, and am grateful for their presence.</p>
<p>I wanted to see if they could talk to me.</p>
<p><img src="/blog_assets/2017/Touche+FernPlantSetup.jpg" alt="Touche+FernPlantSetup"></p>
<p><img src="/blog_assets/2017/Touche+AirPlant-Setup.jpg" alt="Touche+AirPlant-Setup"></p>
<h2 id="the-fern">THE FERN</h2>
<p>I first experimented with the fern. As suggested in the <a href="https://pdfs.semanticscholar.org/bad6/92a87a416a228542f5ed554503b604ad481e.pdf">Botanicus Interacticus paper</a>, I inserted a simple wire lead into the soil of the plant. This would allow the ESP system to measure the conductive profile of the plant as I touch it.</p>
<p>After doing some experiments, I was able to easily train the ESP system to recognize whether I was touching a single leaf:</p>
<p><img src="/blog_assets/2017/fern.gif" alt="fern"></p>
<p>or whether I was lightly caressing down on the top of the leafs with the palm of my hand.</p>
<p><img src="/blog_assets/2017/fern2.gif" alt="fern2"></p>
<p>Here&rsquo;s what that looked like in action:</p>
<p><a href="http://www.youtube.com/watch?v=ZPsU6U54CRM" title=""><img src="http://img.youtube.com/vi/ZPsU6U54CRM/0.jpg" alt=""></a></p>
<p>I also tried experimenting as to whether or not the system was able to detect whether or not I was touching individual leaves, but was not able to get consistent results. I discuss my theory on why this may be the case at the end of this post.</p>
<h2 id="the-air-plant">THE AIR PLANT</h2>
<p>I experimented with the air plant next, and successfully trained the ESP system to be able to discriminate between whether I had my hand at rest on top of the plant:</p>
<p><img src="/blog_assets/2017/airplant+rest.gif" alt="airplant+rest"></p>
<p>and whether or not was &ldquo;tickling&rdquo; the top of the plant</p>
<p><img src="/blog_assets/2017/ariplant+tickle.gif" alt="ariplant+tickle"></p>
<p>Here&rsquo;s what the system looked like in use:</p>
<p><a href="http://www.youtube.com/watch?v=RJ--EB5DpOc" title=""><img src="http://img.youtube.com/vi/RJ--EB5DpOc/0.jpg" alt=""></a></p>
<p>What I was not successful at was training the ESP to discriminate between the act of touching a leaf, and rubbing my finger on a leaf as shown below:</p>
<p><img src="s/blog_assets/2017/leaf+rub.gif" alt="leaf+rub"></p>
<p>I tried moving the alligator clip from one of the leafs to the root - my theory being that perhaps the capacitance wasn&rsquo;t being spread evenly throughout the plant.</p>
<p><img src="/blog_assets/2017/AirPlant+Electrode.jpg" alt="AirPlant+Electrode"></p>
<p><img src="/blog_assets/2017/AirPlant+Electrode2.jpg" alt="AirPlant+Electrode2"></p>
<p>This appeared to have no affect, however.</p>
<p>I was a bit surprised at this - given the subtlety in touch which it seemed Touché was capable of measuring, I had thought the system would be capable of discriminating between touching and rubbing a single leaf. That said, there could be some missing factor (such as amount of training data/sessions) that I&rsquo;m not aware of yet in order to make that happen.</p>
<h2 id="further-learning-goals">FURTHER LEARNING GOALS:</h2>
<h3 id="using-regression">USING REGRESSION</h3>
<p>In the Bottanicus Interactus video (at the time below), the authors show that they are able to determine where at on a long plant stem is being touched, and interact with it in a way that resembles using a slider moving continuously between two points.</p>
<p><a href="http://www.youtube.com/watch?v=EcRSKEIucjk" title=""><img src="http://img.youtube.com/vi/EcRSKEIucjk/0.jpg" alt=""></a></p>
<p>The Touché system uses a Support Vector Machine Learning algorithm, which is capable of both classification and regression; two types of machine learning tasks. In classification, a machine learning system detects what <em>type</em> of events have occurred - in this case, the <em>type of touch</em> that occurred. In regression tasks, a machine learning system maps the distance between two points to control a parameter - so, for instance, you could map the distance travelled by the hand between two points on a plant stem to the value of a volume slider.</p>
<p>in the ESP system, classification is currently supported; regression is not. In order to use Touché to control a continuous stream of value between one point and another, the ESP system would need to be modified to support regression.</p>
<p>(For more info on the principles behind classification and regression, check Rebecca Fiebrink&rsquo;s Kadenze course, <a href="https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info">Machine Learning for Musicians and Artists</a>).</p>
<h3 id="finer-discrimination">FINER DISCRIMINATION</h3>
<p>Determine whether or not it is possible to detect the touches of individual leafs, as opposed to detecting whether &ldquo;a leaf&rdquo; has been touched. It may be that this is possible, but dependent on the type of plant involved - a plant with thicker, more &ldquo;solid&rdquo; leaves may return a conductive pattern that&rsquo;s better at discriminating between individual leaf touches than the thin, loose leaves of the fern plant.</p>
<h3 id="gesture-timeouts">GESTURE TIMEOUTS</h3>
<p>If you watch the videos of the Touché system in action above, you may have noticed that there were occasionally instances in which there is a short &ldquo;bounce&rdquo; during the transition between one gesture class and another. In the Air Plant example, when the hand is moving from a &ldquo;Resting Hand&rdquo; position to a &ldquo;No Hand&rdquo; position, the ESP system will falsely recognize a &ldquo;Tickling&rdquo; gesture.</p>
<p>A potential remedy for this situation is to add a <a href="http://nickgillian.com/grt/api/0.1.0/_class_label_filter_8h.html">Class Label Filter</a> to the ESP&rsquo;s gesture recognition pipeline. This class will allow the system to get rid of the erroneously recognized gesture class. Adding this filter to the ESP pipeline is something I&rsquo;m planning on exploring in future experiments.</p>
<h2 id="code">CODE</h2>
<p>The Processing sketches shown in the videos, and the ESP session data, can be found <a href="https://github.com/narner/Touche-Experiments">here</a>. You&rsquo;ll need to make sure you have <a href="http://processing.org/">Processing</a> installed on your system to run the sketches, and have followed the installation guide for setting the <a href="https://github.com/damellis/ESP">ESP</a> if you want to run the included sessions.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Nick Arner 2020 
  </a>
    <div>













</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
