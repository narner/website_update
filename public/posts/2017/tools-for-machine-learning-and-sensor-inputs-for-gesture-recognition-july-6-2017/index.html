<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Nick Arner  | Tools for Machine Learning and Sensor Inputs for Gesture Recognition</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.68.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.1cb140d8ba31d5b2f1114537dd04802a.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Tools for Machine Learning and Sensor Inputs for Gesture Recognition" />
<meta property="og:description" content="The past several years have seen an explosion in machine learning, including in creative contexts - everything fromhallucinating puppyslugs, togenerating new melodies, machine learning is already beginning to revolutionize the way artists and musicians execute their craft.
My personal interest in the area of machine learning relates to using it to recognize human gestural input via sensors. This interest was sparked from working with Project Soli as a member of the Alpha Developer program." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/2017/tools-for-machine-learning-and-sensor-inputs-for-gesture-recognition-july-6-2017/" />
<meta property="article:published_time" content="2017-06-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2017-06-06T00:00:00+00:00" />
<meta itemprop="name" content="Tools for Machine Learning and Sensor Inputs for Gesture Recognition">
<meta itemprop="description" content="The past several years have seen an explosion in machine learning, including in creative contexts - everything fromhallucinating puppyslugs, togenerating new melodies, machine learning is already beginning to revolutionize the way artists and musicians execute their craft.
My personal interest in the area of machine learning relates to using it to recognize human gestural input via sensors. This interest was sparked from working with Project Soli as a member of the Alpha Developer program.">
<meta itemprop="datePublished" content="2017-06-06T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2017-06-06T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1101">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Tools for Machine Learning and Sensor Inputs for Gesture Recognition"/>
<meta name="twitter:description" content="The past several years have seen an explosion in machine learning, including in creative contexts - everything fromhallucinating puppyslugs, togenerating new melodies, machine learning is already beginning to revolutionize the way artists and musicians execute their craft.
My personal interest in the area of machine learning relates to using it to recognize human gestural input via sensors. This interest was sparked from working with Project Soli as a member of the Alpha Developer program."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="http://example.org/" class="f3 fw2 hover-white no-underline white-90 dib">
      Nick Arner
    </a>
    <div class="flex-l items-center">
      

      
      














    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=http://example.org/posts/2017/tools-for-machine-learning-and-sensor-inputs-for-gesture-recognition-july-6-2017/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=http://example.org/posts/2017/tools-for-machine-learning-and-sensor-inputs-for-gesture-recognition-july-6-2017/&amp;text=Tools%20for%20Machine%20Learning%20and%20Sensor%20Inputs%20for%20Gesture%20Recognition" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://example.org/posts/2017/tools-for-machine-learning-and-sensor-inputs-for-gesture-recognition-july-6-2017/&amp;title=Tools%20for%20Machine%20Learning%20and%20Sensor%20Inputs%20for%20Gesture%20Recognition" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>

      <h1 class="f1 athelas mt3 mb1">Tools for Machine Learning and Sensor Inputs for Gesture Recognition</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2017-06-06T00:00:00Z">June 6, 2017</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>The past several years have seen an explosion in machine learning, including in creative contexts - everything from<a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"> hallucinating puppyslugs</a>, to<a href="https://magenta.tensorflow.org/welcome-to-magenta"> generating new melodies</a>, machine learning is already beginning to revolutionize the way artists and musicians execute their craft.</p>
<p>My personal interest in the area of machine learning relates to using it to recognize human gestural input via sensors. This interest was sparked from <a href="https://www.nickarner.com/project-soli-alpha-developers-program">working with Project Soli as a member of the Alpha Developer program</a>.</p>
<p>Sensors offer a bridge between the physical world and the digital. Rich sensor input combined with machine learning allows for new interfaces to be developed that are novel, expressive, and can be configured to a specialized creative task.</p>
<p>In order to make sense of the potential that sensors offer technologists and artists, it&rsquo;s often necessary to utilize machine learning to build a system that can take advantage of a sensor&rsquo;s capabilities. Sensors by themselves aren&rsquo;t always easy to make sense of right away.</p>
<p>With something like an infrared sensor, you can get a<a href="https://github.com/narner/ProximitySensing-With-Chirpino"> simple interactive system</a> working with a sensor and some if-then-else-that type of logic. Something like LIDAR, or a <a href="https://software.intel.com/en-us/realsense/home">depth-field camera</a>, on the other hand, will have a much larger data footprint - the only way to make sense of the sensor&rsquo;s data is to use machine learning to recognize what patterns are in the real-time data that the sensor is gathering. It’s important to be aware of what type of data a sensor is capable of providing, and whether or not it will be appropriate for the interactive system you are trying to build.</p>
<p>Often, the more complex the data is that the sensor provides, the more interesting things you can do with it.</p>
<p>I wanted to go over some of the open-source tools that are currently available to the creative technology community to leverage the power of sensors for adding gestural interactivity to creative coding projects:</p>
<ul>
<li>Wekinator</li>
<li>Gesture Recognition Toolkit / ofxGrt</li>
<li>ESP</li>
</ul>
<h2 id="wekinator">WEKINATOR</h2>
<p>The<a href="http://www.wekinator.org/"> Wekinator</a> is a machine-learning middleware program developed by<a href="http://www.doc.gold.ac.uk/~mas01rf/Rebecca_Fiebrink_Goldsmiths/welcome.html"> Dr. Rebecca Fiebrink</a> of Goldsmith&rsquo;s University in London. The basic idea of its use is that it receives data from a sensor via<a href="http://opensoundcontrol.org/"> OSC (open-sound control)</a> from a program that’s acquiring the data from the sensor, such as an Arduino or a Processing sketch. Wekinator is used to train a machine learning system on this incoming data to recognize which gesture has occurred, or to map the start and end times of a gesture to a value that can be used to control a parameter range. These values are sent out from Wekinator via OSC, and can then be received by a program that maps those values to to control audio/visual elements.</p>
<p><a href="http://www.wekinator.org/examples/">Lots of examples</a> are provided showing how to use a plethora of sensors with the Wekinator, such as a Wii-Mote, Kinect, Leap Motion, etc, and map them to parameters various receiver programs.</p>
<p>What&rsquo;s great about the Wekinator is that you don&rsquo;t have to know much about how machine learning works in order to use it - its strength is in the fact that it&rsquo;s user friendly and easy to experiment with quickly.</p>
<p>If you&rsquo;re interested in exploring how you can use Wekinator to add interactivity to your projects, I highly recommend Dr. Fiebrink&rsquo;s<a href="https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info"> Machine Learning for Artists and Musicians</a> course on Kadenze.</p>
<h3 id="heading"></h3>
<h2 id="gesture-recognition-toolkit">GESTURE RECOGNITION TOOLKIT</h2>
<p>The<a href="https://github.com/nickgillian/grt"> GRT</a> is a cross-platform toolkit for interfacing with sensors for gesture-recognition systems. It&rsquo;s developed and maintained by<a href="http://nickgillian.com/"> Nick Gillian</a>, who is currently a lead machine learning researcher for Google&rsquo;s<a href="https://atap.google.com/soli/"> Project Soli</a>.</p>
<p>The GRT can be used as a<a href="http://www.nickgillian.com/wiki/pmwiki.php/GRT/GUI"> GUI-based application</a> that acts as middleware between your sensor input via OSC, and a receiver program that reacts to the gesture events detected by your sensor. It’s useage follows the same pattern as the Wekinator.</p>
<p>However, the real benefit of the GRT is how you can write your gesture-recognition pipeline, train your model, and use your code on multiple platforms. This is useful if you want to prototype a gesture-recognition system on your desktop that may need to be deployed on custom, embedded hardware. Additionally, you can write a custom module for the GRT in order to customize your pipeline based on some unique characteristics of the sensor you&rsquo;re using.</p>
<p>Be sure to read Nick’s <a href="http://jmlr.org/papers/volume15/gillian14a/gillian14a.pdf">paper</a> on the toolkit in the Journal of Machine Learning Research.</p>
<h3 id="ofxgrt">OFXGRT</h3>
<p>The GRT also comes embedded in a wrapper for use in Open Frameworks, a C++ toolkit for creative coding,<a href="https://github.com/nickgillian/ofxGrt"> ofxGrt</a>. This type of wrapper is known as an addon. This makes it extremely easy to integrate into new or existing Open Frameworks projects. Combined with the numerous other Open Frameworks<a href="http://ofxaddons.com/categories"> addons</a>, ofxGrt allows coders to integrate with various types of sensors for adding an interface to physical world with their creative projects.</p>
<h2 id="example-based-sensor-predictions">EXAMPLE-BASED SENSOR PREDICTIONS</h2>
<p>The<a href="https://github.com/damellis/ESP"> Example-based Sensor Predictions</a> project by<a href="http://alumni.media.mit.edu/~mellis/"> David Mellis</a> and<a href="https://www.benzhang.name/"> Ben Zhang</a> was created to make sensor-based machine-learning systems accessible to the Maker and Arduino community. Though the maker community was very familiar with how to work with sensors, fully utilizing their potential for rich interactions really wasn&rsquo;t possible to do in the Arduino ecosystem until this project was developed.</p>
<p>Here’s a short video overview of the project from the creators:</p>
<p><a href="http://www.youtube.com/watch?v=5nDCG4vkFP0" title=""><img src="http://img.youtube.com/vi/5nDCG4vkFP0/0.jpg" alt=""></a></p>
<p>The project is built so that users can interface with sensors via Processing, and the gesture recognition pipeline is built using the GRT. It contains four examples:</p>
<ul>
<li>Audio Detection</li>
<li>Color Detection</li>
<li>Gesture Recognition</li>
<li>Touché touch detection</li>
</ul>
<p>Developers are also given API documentation for how to write their own sensor-based gesture recognition examples as part of the ESP environment.</p>
<p>I&rsquo;ve recently been doing some experiments with the ESP project, and hope to share some of those examples soon.</p>
<p>There’s still a lot of work to be done exploring how new sensing technologies can be leveraged to improve the way that people interact with the devices around them. In order to understand how these technologies may be useful, artists and musicians need to be at the forefront of using them&hellip;pushing sensors and machine learning systems to the limits of how they can be used in expressive contexts. When artists push interfaces to their full potential, everyone benefits as a result - as Bill Buxton said in Artists and the Art of the Luthier, “I also discovered that in the grand scheme of things, there are three levels of design: standard spec., military spec., and artist spec. Most significantly, I learned that the third was the hardest (and most important), but if you could nail it, then everything else was easy.”</p>
<p>The tools described above should encourage artists, musicians, designers, and technologists to explore using sensor-based gesture recognition systems in their creative practice, and imagine ways in which interactivity can be added to new products, pieces, designs, and compositions.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Nick Arner 2020 
  </a>
    <div>













</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
