<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Nick Arner  | Machine-Learning powered Gesture Recognition on iOS</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.68.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.1cb140d8ba31d5b2f1114537dd04802a.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Machine-Learning powered Gesture Recognition on iOS" />
<meta property="og:description" content="It’s almost hard not to be reading about machine learning these days — and that trend will only increase. Machine learning is opening up powerful new capabilities for smartphone apps, from image classification to facial recognition.
It’s also capable of identifying how someone is interacting with their smartphone.
This project shows how to use the Gesture Recognition Toolkit to detect how a user is moving their phone through physical space. (I wrote up a quick guide here on how to begin integrating the GRT into an iPhone app project)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/2017/machine-learning-powered-gesture-recognition-on-ios-octobober-7-2017/" />
<meta property="article:published_time" content="2017-10-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2017-10-07T00:00:00+00:00" />
<meta itemprop="name" content="Machine-Learning powered Gesture Recognition on iOS">
<meta itemprop="description" content="It’s almost hard not to be reading about machine learning these days — and that trend will only increase. Machine learning is opening up powerful new capabilities for smartphone apps, from image classification to facial recognition.
It’s also capable of identifying how someone is interacting with their smartphone.
This project shows how to use the Gesture Recognition Toolkit to detect how a user is moving their phone through physical space. (I wrote up a quick guide here on how to begin integrating the GRT into an iPhone app project).">
<meta itemprop="datePublished" content="2017-10-07T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2017-10-07T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="985">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine-Learning powered Gesture Recognition on iOS"/>
<meta name="twitter:description" content="It’s almost hard not to be reading about machine learning these days — and that trend will only increase. Machine learning is opening up powerful new capabilities for smartphone apps, from image classification to facial recognition.
It’s also capable of identifying how someone is interacting with their smartphone.
This project shows how to use the Gesture Recognition Toolkit to detect how a user is moving their phone through physical space. (I wrote up a quick guide here on how to begin integrating the GRT into an iPhone app project)."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="http://example.org/" class="f3 fw2 hover-white no-underline white-90 dib">
      Nick Arner
    </a>
    <div class="flex-l items-center">
      

      
      














    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=http://example.org/posts/2017/machine-learning-powered-gesture-recognition-on-ios-octobober-7-2017/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=http://example.org/posts/2017/machine-learning-powered-gesture-recognition-on-ios-octobober-7-2017/&amp;text=Machine-Learning%20powered%20Gesture%20Recognition%20on%20iOS" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://example.org/posts/2017/machine-learning-powered-gesture-recognition-on-ios-octobober-7-2017/&amp;title=Machine-Learning%20powered%20Gesture%20Recognition%20on%20iOS" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>

      <h1 class="f1 athelas mt3 mb1">Machine-Learning powered Gesture Recognition on iOS</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2017-10-07T00:00:00Z">October 7, 2017</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>It’s almost hard not to be reading about machine learning these days — and that trend will only increase. Machine learning is opening up powerful new capabilities for smartphone apps, from image classification to facial recognition.</p>
<p>It’s also capable of identifying how someone is interacting with their smartphone.</p>
<p>This project shows how to use the <a href="https://github.com/nickgillian/grt">Gesture Recognition Toolkit</a> to detect how a user is moving their phone through physical space. (I wrote up a quick guide <a href="https://medium.com/@narner/integrating-the-grt-into-an-iphone-app-part-one-1b12dc69c5bc">here</a> on how to begin integrating the GRT into an iPhone app project).</p>
<p>The iPhone’s accelerometer will be used as the input for the gesture recognition system. This all for the app to extract gestural intent from the way the iPhone is moved by the user. The data will be fed into a classifier that’s part of a GRT pipeline, which will predict what gesture the user is performing. To access this sensor data, the app will make use of Apple’s <a href="https://developer.apple.com/documentation/coremotion">CoreMotion Framework</a>. The fine folks at NSHipster have a good overview of the framework <a href="http://nshipster.com/cmdevicemotion/">here</a>.</p>
<p>The app will have two modes, and two view controllers accordingly. The first one, shown below, is the training mode. While we’re making the corresponding gesture with the phone, the red “Train” button is held down. While it’s held down, data from the phone’s accelerometer is being saved to a data structure that we’ll use to train our pipeline. As soon as the button is let go from being held down, that process is stopped. A segment controller will serve as a way to set the class-label for the gesture that’s being performed.</p>
<p><img src="/Users/nickarner/Documents/MyWebsite/static/blog_assets/2017/Training+VC.jpg" alt="Training+VC"></p>
<p>The second View Controller is the real-time prediction mode. When we get to this screen, our pipeline will be quickly trained on the data structure.</p>
<p>When gestures are performed, the app will display the predicted gesture output on the screen, as well as print the predicted class to the console when in debug mode.</p>
<p><img src="/Users/nickarner/Documents/MyWebsite/static/blog_assets/2017/Prediction+VC.jpg" alt="Prediction+VC"></p>
<p>The App Delegate contains a global instance of a GRT pipeline, so that it can be accessed by both View Controllers:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-swift" data-lang="swift"><span style="color:#66d9ef">var</span> pipeline: GestureRecognitionPipeline?

<span style="color:#66d9ef">func</span> <span style="color:#a6e22e">application</span>(<span style="color:#66d9ef">_</span> application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplicationLaunchOptionsKey: Any]?) -&gt; Bool {

    <span style="color:#75715e">//Create an instance of a gesture recognition pipeline to be used as a global variable, accesible by both our training and prediction view controllers</span>
    <span style="color:#66d9ef">self</span>.pipeline = GestureRecognitionPipeline()

    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">true</span>
}
</code></pre></div><p>An AccelerometerManager class will allow the two View Controllers to have access to the X, Y, and Z coordinates from the phone’s accelerometer. The start method, shown below, will create an object that will return the accelerometer data:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-swift" data-lang="swift"><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">start</span>(<span style="color:#66d9ef">_</span> accHandler: @escaping (<span style="color:#66d9ef">_</span> x: Double, <span style="color:#66d9ef">_</span> y: Double, <span style="color:#66d9ef">_</span> z: Double) -&gt; Void) {
    <span style="color:#66d9ef">let</span> handler: CMAccelerometerHandler  = {(data: CMAccelerometerData?, error: Error?) -&gt; Void <span style="color:#66d9ef">in</span>
        <span style="color:#66d9ef">guard</span> <span style="color:#66d9ef">let</span> acceleration = data?.acceleration <span style="color:#66d9ef">else</span> {
            print(<span style="color:#e6db74">&#34;Error: data is nil: </span><span style="color:#e6db74">\(</span>String<span style="color:#e6db74">(</span>describing: error<span style="color:#e6db74">))</span><span style="color:#e6db74">&#34;</span>)
            <span style="color:#66d9ef">return</span>
        }

        accHandler(acceleration.x, acceleration.x, acceleration.z)
    } 

    motionManager.startAccelerometerUpdates(to: motionQueue, withHandler: handler)
}
</code></pre></div><p>The stop method will halt this process, so that the app is only gathering data from the phone’s accelerometer when the pipeline is being trained, or performing real-time recognition.</p>
<h2 id="training-view-controller">TRAINING VIEW CONTROLLER</h2>
<p>The heart of the Training View Controller is the <code>TrainBtnPressed</code> function. It’s called whenever the red “Train” button is held down, takes the accelerometer coordinates, and creates a vector out of them. This vector is then sent to the pipeline wrapper’s <code>addSamplesToClassificationData</code> function.</p>
<pre><code>func TrainBtnPressed(_ sender: Any) {
        trainButton.isSelected = true
        let gestureClass = self.gestureSelector.selectedSegmentIndex

        accelerometerManager.start({ (x, y, z) -&gt; Void in

            //Add the accellerometer data to a vector, which is how we'll store the classification data
            let vector = VectorFloat()
            vector.clear()
            vector.pushBack(x)
            vector.pushBack(y)
            vector.pushBack(z)

            print(&quot;x&quot;, x)
            print(&quot;y&quot;, y)
            print(&quot;z&quot;, z)
            print(&quot;Gesture class is %@&quot;, gestureClass);
            self.pipeline!.addSamplesToClassificationData(forGesture: UInt(gestureClass), vector)
        })
    }
</code></pre><p>Once the gesture data has been recorded, and the “Save Pipeline” button is pressed, the pipeline and gesture training data is saved.</p>
<h2 id="prediction-view-controller">PREDICTION VIEW CONTROLLER</h2>
<p>Once the pipeline has been saved, the app can be used in real-time prediction mode. This view controller will take the accelerometer data, and pass that to the pipeline using the “predict” function.</p>
<p>What’s happening here is that the pipeline is comparing the incoming accelerometer data against the labeled training data, and making a prediction as to what gesture the user has made with their phone. This data is represented with the classLabel property.</p>
<p>The app will first make sure sure that it can successfully load the <code>train.grt</code> pipeline file and <code>trainingData.csv</code> classification data files. If it can, then a call to the pipeline’s <code>train</code> method is made, which is located in the Objective-C wrapper for the GRT:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-swift" data-lang="swift"><span style="color:#f92672">-</span> (BOOL)trainPipeline {
    <span style="color:#f92672">*</span><span style="color:#66d9ef">self</span>.trainingData = <span style="color:#66d9ef">self</span>.classificationData-&gt;split(<span style="color:#ae81ff">80</span>);
    BOOL trainSuccess = <span style="color:#66d9ef">self</span>.instance-&gt;train( <span style="color:#f92672">*</span>(<span style="color:#66d9ef">self</span>.trainingData) );
    ...
    <span style="color:#66d9ef">return</span> trainSuccess;
}
</code></pre></div><p>Back in the Prediction Controller, the performGesturePrediction() method takes the real-time accelerometer coordinates, and puts them inside of a Vector:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-swift" data-lang="swift"><span style="color:#66d9ef">func</span> <span style="color:#a6e22e">performGesturePrediction</span>() {
        accelerometerManager.start { (x, y, z) -&gt; Void <span style="color:#66d9ef">in</span>
            <span style="color:#66d9ef">self</span>.vector.clear()
            <span style="color:#66d9ef">self</span>.vector.pushBack(x)
            <span style="color:#66d9ef">self</span>.vector.pushBack(y)
            <span style="color:#66d9ef">self</span>.vector.pushBack(z)
            ...
</code></pre></div><p>This vector is then passed to the pipeline’s Objective-C wrapper:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-swift" data-lang="swift"><span style="color:#66d9ef">self</span>.pipeline?.predict(<span style="color:#66d9ef">self</span>.vector)
</code></pre></div><p>The remainder of the function gets the predicted class label from the pipeline, and displays the predicted gesture result in the user interface:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-swift" data-lang="swift">...
            DispatchQueue.main.async {
                <span style="color:#66d9ef">self</span>.predictedGestureLabel.text = String(describing:                 <span style="color:#66d9ef">self</span>.pipeline?.predictedClassLabel ?? <span style="color:#ae81ff">0</span>)
            }

            print(<span style="color:#e6db74">&#34;PREDICTED GESTURE&#34;</span>, <span style="color:#66d9ef">self</span>.pipeline?.predictedClassLabel ?? <span style="color:#ae81ff">0</span>);
        }
    }
</code></pre></div><h2 id="using-the-app">USING THE APP</h2>
<p>One important consideration to keep in mind when implementing a real-time gesture recognition system is to train the system what <em>not</em> to recognize. In this case, the resting state of the phone (when a gesture isn’t being performed with it) is when the phone is held stationary:</p>
<p><img src="/Users/nickarner/Documents/MyWebsite/static/blog_assets/2017/StationaryPosition.jpg" alt="StationaryPosition"></p>
<p>​</p>
<p>I was able to train the gesture recognition system to be able to recognize a rapid shaking of the phone:</p>
<p><img src="/Users/nickarner/Documents/MyWebsite/static/blog_assets/2017/Gesture1.gif" alt="Gesture1"></p>
<p>…a side swipe:</p>
<p><img src="/Users/nickarner/Documents/MyWebsite/static/blog_assets/2017/Swipe+Gesture.gif" alt="Swipe+Gesture"></p>
<p>…and a “wave motion”:</p>
<p><img src="https://static1.squarespace.com/static/52e0ac24e4b073428e07d275/t/59d8f95f197aea8dd9452b65/1507391879410/Wave+gesture?format=1500w" alt="img"></p>
<p>If you end up using this demo to recognize some interesting gestures, please feel free to share your results in the comments! Remember that the robustness of the gesture recognition system will depend on how well you train for the specific gestures that you want to be recognized.</p>
<p>The project code is on GitHub <a href="https://github.com/narner/GRT-iOS-HelloWorld">here</a>.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Nick Arner 2020 
  </a>
    <div>













</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
