<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Classification of Sound Files on iOS with the SoundAnalysis Framework and ESC-10 CoreML Model - Nick Arner</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Classification of Sound Files on iOS with the SoundAnalysis Framework and ESC-10 CoreML Model" />
<meta property="og:description" content="Earlier this year, Apple introduced the SoundAnalysis framework, enabling apps to “analyze streamed and file-based audio to classify it as a particular type”.
It’s available in the following SDK’s:
 iOS13&#43; macOS 10.15&#43; Mac Catalyst 13.0&#43; tvOS 13.0&#43; watchOS 6.0&#43;  INPUT TYPES The SoundAnalysis framework is able to work with live audio via the device microphone, or from pre-recorded audio files. In a real-world application scenario, these could be either audio files downloaded from a server onto the app, or recorded by the app for later analysis." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/notes/classification-of-sound-files-on-ios-with-the-soundanalysis-framework-and-esc-10-coreml-model-october-29-2019/" />
<meta property="article:published_time" content="2019-10-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-10-29T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Classification of Sound Files on iOS with the SoundAnalysis Framework and ESC-10 CoreML Model"/>
<meta name="twitter:description" content="Earlier this year, Apple introduced the SoundAnalysis framework, enabling apps to “analyze streamed and file-based audio to classify it as a particular type”.
It’s available in the following SDK’s:
 iOS13&#43; macOS 10.15&#43; Mac Catalyst 13.0&#43; tvOS 13.0&#43; watchOS 6.0&#43;  INPUT TYPES The SoundAnalysis framework is able to work with live audio via the device microphone, or from pre-recorded audio files. In a real-world application scenario, these could be either audio files downloaded from a server onto the app, or recorded by the app for later analysis."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />

	
	<script src="/js/main.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title"><a href="/">Nick Arner</a></h1>
	<div class="site-description"><nav class="nav social">
			<ul class="flat"></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Classification of Sound Files on iOS with the SoundAnalysis Framework and ESC-10 CoreML Model</h1>
			<div class="meta">Posted at &mdash; Oct 29, 2019</div>
		</div>

		<div class="markdown">
			<p>Earlier this year, Apple introduced the <a href="https://developer.apple.com/documentation/soundanalysis">SoundAnalysis framework</a>, enabling apps to “analyze streamed and file-based audio to classify it as a particular type”.</p>
<p>It’s available in the following SDK’s:</p>
<ul>
<li>iOS13+</li>
<li>macOS 10.15+</li>
<li>Mac Catalyst 13.0+</li>
<li>tvOS 13.0+</li>
<li>watchOS 6.0+</li>
</ul>
<h3 id="input-types"><strong>INPUT TYPES</strong></h3>
<p>The SoundAnalysis framework is able to work with live audio via the device microphone, or from pre-recorded audio files. In a real-world application scenario, these could be either audio files downloaded from a server onto the app, or recorded by the app for later analysis.</p>
<p>However, at the time of writing, I was not able to get live audio input working with Sound Analysis after following Apple’s guide. Here’s a <a href="https://stackoverflow.com/questions/58496448/error-updating-tree-format-when-using-ios-soundanalysis-framework?noredirect=1#comment103345155_58496448">StackOverflow post</a> with what I was working through for future reference.</p>
<p>Should I get live audio input working, I’ll update the blog post / project with an additional example. For now, this project demonstrates how to use SoundAnalysis off of audio files only.</p>
<h3 id="dataset"><strong>DATASET</strong></h3>
<p>The dataset I used is the ESC (environmental sound classification)-10 dataset; the same one that Apple uses in their <a href="https://apple.github.io/turicreate/docs/userguide/sound_classifier/">TuriCrate tutorial on Sound Classification</a>. The ESC-10 is a subset of the <a href="https://github.com/karoldvl/ESC-50">larger ESC-50 dataset that’s available on GitHub</a>.</p>
<p>The dataset consists of ten classes, with ten files for each class. The classes are:</p>
<ol>
<li>Dog Barking</li>
<li>Rain</li>
<li>Sea Waves</li>
<li>Crying baby</li>
<li>Clock Tick</li>
<li>Person Sneezing</li>
<li>Helicopter</li>
<li>Chainsaw</li>
<li>Rooster</li>
<li>Fire Crackling</li>
</ol>
<p>This is not a very large collection of audio files for an extremely robust dataset, and the classes aren’t particularly interesting in and of themselves without a larger context — the point of the project is to demonstrate the concept of how a sound classification dataset would be used with CreateML to create a CoreML model in conjunction with the SoundAnalysis framework. If you were building a custom sound classification recognition model for use in a production app, you would most likely need to create your own custom dataset that uses audio data that is close to the real world scenario in which your app would be used for.</p>
<p>There are some other audio-focused datasets available; a compilation of some can be found <a href="http://www.cs.tut.fi/~heittolt/datasets">here</a>, <a href="https://towardsdatascience.com/a-data-lakes-worth-of-audio-datasets-b45b88cd4ad">here</a>, and <a href="https://cassebook.github.io/ch06/index/">here</a>. However, you’ll need to make sure that you check what the licenses are for each dataset before you use them to create a model that can be used in a commercial application.</p>
<h3 id="createml-project"><strong>CREATEML PROJECT</strong></h3>
<p>The <a href="https://developer.apple.com/documentation/createml">CreateML app</a> that’s included in Xcode allows developers to create CoreML models that recognize sounds without writing any code.</p>
<p>With this <a href="https://developer.apple.com/videos/play/wwdc2019/425/">new template from CreateML</a>, you can make a machine learning model that can recognize types of sound events without having to write any code, and use the interface to do so in CreateML that you would use to make ML models that are trained on visual, tabular, or sensor data.</p>
<p>In addition to the ECS-10 CoreML model, I’ve included the CreateML project used to make the model in this repository.</p>
<p><img src="/blog_assets/2019/CreateMLInterface.jpg" alt="CreateMLInterface"></p>
<p>​		CreateML Training Interface</p>
<p>The ECS-10-CoreML-Demo is an Xcode project that demonstrates how to use the SoundAnalysis framework in an app.</p>
<p>The app will randomly select from ten pre-loaded .wav files and play them through the speakers (using <a href="http://audiokit.io/">AudioKit</a>), while running an analysis with the SoundAnalyzer framework. Setting up this pipeline takes remarkably few lines of code:</p>
<pre><code>var model: MLModel!
    
//Access the bundled CoreML model
let soundClassifier = ESC_10_Sound_Classifier()
model = soundClassifier.model

// Create a new audio file analyzer.
do {
 audioFileAnalyzer = **try** SNAudioFileAnalyzer(url: audioFileURL)
} catch

// Create a new observer that will be notified of analysis results.
let resultsObserver = ResultsObserver()
// Prepare a new request for the trained model.

do {
 let request = try SNClassifySoundRequest(mlModel: model)
 try audioFileAnalyzer.add(request, withObserver: resultsObserver)
} catch

// Analyze the audio data.
audioFileAnalyzer.analyze()
</code></pre><h3 id="heres-a-video-of-the-test-app-in-action"><strong>HERE’S A VIDEO OF THE TEST APP IN ACTION:</strong></h3>
<p>The project can be found on GitHub <a href="https://github.com/narner/ESC10-CoreML">here</a>.</p>
<p><a href="http://www.youtube.com/watch?v=dAtzSo51T_4" title=""><img src="http://img.youtube.com/vi/dAtzSo51T_4/0.jpg" alt=""></a></p>

		</div>

		<div class="post-tags">
			
				
			
		</div>
		</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div>	
	</nav>
</div>




</body>
</html>
